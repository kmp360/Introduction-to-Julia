{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmp 2022\n",
    "\n",
    "Based on **Lauwens & Downey \"Think Julia: How to Think Like a Computer Scientist\" \n",
    "https://benlauwens.github.io/ThinkJulia.jl/latest/book.html**\n",
    "\n",
    "Resources:\n",
    "\n",
    "Julia webpage https://julialang.org/ \n",
    "\n",
    "Julia documentation https://docs.julialang.org/en/v1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 13 -- Data structures -- Case studies\n",
    "\n",
    "https://benlauwens.github.io/ThinkJulia.jl/latest/book.html#chap13\n",
    "\n",
    "At this point you have learned about Julia’s core data structures, and you have seen some of the algorithms that use them. This chapter presents a case study with exercises that let you think about choosing data structures and practice using them.\n",
    "\n",
    "### A usful detour\n",
    "\n",
    "The alphabet -- ways of \"stacking\" ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1 = ['a' : 'z', 'A':'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = ['a' : 'z'; 'A':'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha2 = ['a' : 'z' 'A':'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_index = Dict(zip(alphabet, Int.(alphabet)))\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_index = Dict(zip(string.(alphabet), Int.(alphabet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **isletter** tests whether a character is alphabetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built in version\n",
    "isletter('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isletter(\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function is_letter(chr::String)\t\n",
    "    alphabet = string.(['a' : 'z'; 'A':'Z'; '-'; ' '])\n",
    "    return chr in alphabet\n",
    "end\n",
    "\n",
    "is_letter(\"B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us download \"Emma\" by Jane Austen (https://ia800303.us.archive.org/24/items/EmmaJaneAusten_753/) from the Internet Archive (https://ia800303.us.archive.org/) in raw form and see what it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw read of Emma\n",
    "\n",
    "em2 = let\n",
    "\turl = \"https://ia800303.us.archive.org/24/items/EmmaJaneAusten_753/emma_pdf_djvu.txt\"\n",
    "\traw_text = read(download(url), String);\n",
    "\tlowercase.(raw_text)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13-1\n",
    "\n",
    "Write a program that reads a file, breaks each line into words, strips whitespace and punctuation from the words, and converts them to lowercase. Clearly we are going to need to do some pre-processing of the raw data (\"data wrangling\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function preprocess(text::String)\n",
    "    # see info on regular expressions\n",
    "    # https://docs.julialang.org/en/v1/manual/strings/#Regular-Expressions\n",
    "    # http://www.pcre.org/current/doc/html/pcre2syntax.html\n",
    "\n",
    "    # clean up whitespace\n",
    "    text = replace(text, r\"\\s+\" => \" \")\n",
    "\n",
    "    # clean up broken words: \"com-\", \"fortable\"\n",
    "    text = replace(text, r\"-\\s+\" => \"\")\n",
    "\n",
    "    # clean up other characters found by random inspection\n",
    "    text = replace(text, r\"(\\.|,|;|:|\\?|!|'|\\-|—|\\^|)\" => \"\")\n",
    "    text = replace(text, r\"(\\[|\\]|\\)|\\(|\\{|\\}|•|\\*|\\<|\\>)\" => \"\")\n",
    "    text = replace(text, r\"(°|■|©|»|«|♦|•|\\\"|£|\\$|&|\\/)\" => \"\")\n",
    "    text = replace(text, r\"(\\+|\\#|§)\" => \"\")\n",
    "    text = replace(text, r\"([0-9])\" => \"\")\n",
    "\n",
    "    #generate a CSV text \n",
    "    cleantext = replace(text, r\"(\\s|\\b)\" => \",\")\n",
    "\n",
    "    return cleantext \t# sort of ...\n",
    "end\n",
    "\n",
    "function splt(txt::String)\n",
    "    # split on whitespace or other word boundaries\n",
    "    # and type-cast the Array of Substrings from split into an Array of Strings\n",
    "    # tokens = string.(split(cleantext, r\"(\\s|\\b)\"))\n",
    "\n",
    "    return tokens = string.(split(txt, \",\"))\n",
    "end\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "words_em2 = splt(preprocess(em2))  # still not good enough\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency analysis\n",
    "\n",
    "### Exercise 13-2\n",
    "\n",
    "Let's continue with Emma, but if you prefer, you could go to Project Gutenberg (https://gutenberg.org) and download your favorite out-of-copyright book in plain text format. \n",
    "\n",
    "Here is a slightly different approach to the initial `data wrangling` step which skips over the header information and other irrelevant information at the beginning and the end of the file, and process the rest of the words as before.\n",
    "\n",
    "Note the management of unwanted features like com- \\nfortable etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = let\n",
    "\t\n",
    "\turl = \"https://ia800303.us.archive.org/24/items/EmmaJaneAusten_753/emma_pdf_djvu.txt\"\n",
    "\n",
    "\traw_text = read(download(url), String);\n",
    "\t\n",
    "\tfirst_words = \"Emma Woodhouse\";\n",
    "\tlast_words = \"THE END\";\n",
    "\t\n",
    "\tstart_index = findfirst(first_words, raw_text)[1]\n",
    "\tstop_index = findlast(last_words, raw_text)[end]\n",
    "\t\n",
    "\tlowercase.(raw_text[start_index:stop_index])\n",
    "\t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function splitwords(text::String)\n",
    "\t# see info on regular expressions\n",
    "\t# https://docs.julialang.org/en/v1/manual/strings/#Regular-Expressions\n",
    "\t# http://www.pcre.org/current/doc/html/pcre2syntax.html\n",
    "\t\n",
    "\t# clean up whitespaces\n",
    "\ttext = replace(text, r\"\\s+\" => \" \")\n",
    "\t\n",
    "\t# clean up broken words: \"com-\", \"fortable\"\n",
    "\ttext = replace(text, r\"-\\s+\" => \"\")\n",
    "\ttext = replace(text, r\"(\\.|,|;|:|\\?|!|'|\\-|—|\\^|§)\" => \"\")\n",
    "\ttext = replace(text, r\"(\\[|\\]|\\)|\\(|\\{|\\}|•|\\*|\\<|\\>)\" => \"\")\n",
    "\ttext = replace(text, r\"(°|■|©|»|«|♦|•|\\\"|£|\\$|&|\\/)\" => \"\")\n",
    "\ttext = replace(text, r\"x*\" => \"\")\n",
    "\ttext = replace(text, r\"([0-9])\" => \"\")\n",
    "\t\n",
    "\t#text = replace(text, r\"(\\s|\\b)\" => \",\")\n",
    "\tcleantext = replace(text, r\"(\\s|\\b)\" => \",\")\n",
    "\t#cleantext = replace(text, r\"\\\\\" => \"\")\n",
    "\n",
    "\t# split on whitespace or other word boundaries\n",
    "\t# and type-cast the Array of Substrings from split into an Array of Strings\n",
    "\t#tokens = String.(split(cleantext, r\"(\\s|\\b)\"))\n",
    "\ttokens = string.(split(cleantext, \",\"))\n",
    "end\n",
    "\n",
    "emma_words = splitwords(emma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the program to count the total number of words in the book, and the number of times each word is used. Print the number of different words used in the book. You could compare different books by different authors, written in different eras. Who of the authors you looked at uses the most extensive vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function word_counts(words::Array{String, 1})\n",
    "\t\n",
    "\tcounts = Dict{String,Int}()\n",
    "\n",
    "\tfor word in words\n",
    "\t\t\n",
    "\t\tif word == \"\" || word == \"\\\\\"\n",
    "\t\t\t# do nothing\n",
    "\t\telse\n",
    "\t\t\tcounts[word] = get!(counts, word, 0) + 1\n",
    "\t\tend\n",
    "\tend\n",
    "\t\n",
    "\treturn counts\n",
    "end\n",
    "\n",
    "emma_dictionary = word_counts(emma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function reverse_dictionary(d::Dict)\n",
    "\tD = Dict{Int,Array{String,1}}()\n",
    "\t\n",
    "\tfor key in keys(d)\n",
    "\t\t\n",
    "\t\tD[d[key]] = push!( get!(D, d[key], []) , key)\n",
    "\tend\n",
    "\t\n",
    "\treturn D\n",
    "end\n",
    "\n",
    "reverse_emma_dictionary = reverse_dictionary(emma_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13-3\n",
    "\n",
    "Modify the program from the previous exercise to print the 10 most frequently used words in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sort_on_vs(d::Dict, rev=true)\n",
    "    ks = collect(keys(d))\n",
    "    vs = collect(values(d))\n",
    "\n",
    "    if rev \n",
    "        sindex = sortperm(vs, rev=true) \t\n",
    "    else\n",
    "        sindex = sortperm(vs, rev=false)\n",
    "    end\n",
    "\n",
    "    sorted_ks = ks[sindex]\t# keys, values returned in the same order\n",
    "    sorted_vs = vs[sindex]\n",
    "\n",
    "    return collect(zip(sorted_ks, sorted_vs))\n",
    "end\n",
    "\n",
    "#---\n",
    "\n",
    "N = 10\n",
    "\t\t\n",
    "list = sort_on_vs(emma_dictionary)[1:N]\n",
    "\n",
    "println(\"List of the $N most common words in \", \"Emma\")\n",
    "println(\"----------------------------------------\")\n",
    "\n",
    "for k = 1:length(list)\n",
    "    println(\"$k.\", \"\\t \", list[k][1], \"   \\t\\t\", list[k][2])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13-4\n",
    "\n",
    "Modify the previous program to read a word list and then print all the words in the book that are not in the word list. How many of them are typos? How many of them are common words that should be in the word list, and how many of them are really obscure?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = let  # note that the output of a let-block is assigned to a variable\n",
    "\t\n",
    "\twords = Dict{String, Any}()\n",
    "\t# note the escape character \\ in \\\\\n",
    "\tfullpath = \"E:\\\\Julia\\\\a-Pluto-NoteBooks\\\\ThinkJulia-notebooks\\\\words.txt\"\n",
    "\tinfile = open(fullpath)\n",
    "\n",
    "\tfor line in eachline(infile)\n",
    "\t\twords[line] = []\n",
    "\tend\n",
    "\t\n",
    "\tclose(infile)\n",
    "\twords\n",
    "\t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function dictionary_difference(book::Dict, list::Dict)\n",
    "\t\n",
    "    (ismissing(book) || ismissing(wordlist)) && return missing  # guard\n",
    "\n",
    "    words_not_in_list = Array{String, 1}()\n",
    "\n",
    "    for word in keys(book)\n",
    "\n",
    "        if word in keys(list)\n",
    "            # do nothing\n",
    "        else\n",
    "            push!(words_not_in_list, word)\n",
    "        end\t\t\n",
    "    end\t\t\n",
    "    return words_not_in_list\n",
    "end\n",
    "\n",
    "#---\n",
    "\n",
    "nonwords = dictionary_difference(emma_dictionary, wordlist)\n",
    "sort!(nonwords)\n",
    "\n",
    "for nonword in nonwords\n",
    "    if length(nonword) < 2\n",
    "        println()\n",
    "    end\n",
    "    println(nonword)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random numbers\n",
    "\n",
    "Given the same inputs, most computer programs generate the same outputs every time, so they are said to be deterministic. For some applications, though, we want the computer to be unpredictable. Making a program nondeterministic turns out to be difficult, but there are ways to make it at least behave as if nondeterministic. One of them is to use algorithms that generate **pseudorandom numbers**. \n",
    "\n",
    "Pseudorandom numbers are not truly random because they are generated by a deterministic computation, but just by looking at the numbers it is all but impossible to distinguish them from random. \n",
    "\n",
    "The function **rand** returns a random float64 between 0.0 and 1.0 (including 0.0 but not 1.0). Each time you call `rand`, you get the next number in a long series. To see a sample, run this loop:\n",
    "\n",
    "```Julia\n",
    "\tfor i in 1:10\n",
    "\t\tx = rand()\n",
    "\t\tprintln(x)\n",
    "\tend\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Pseudorandom numbers generated by rand()\")\n",
    "println(\"----------------------------------------\")\n",
    "for i in 1:10\n",
    "    x = rand()\n",
    "    println(i, \".\\t\\t\", x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **rand** can take an **iterator or array as argument** and returns a random element:\n",
    "\n",
    "```Julia\n",
    "\tfor i in 1:10\n",
    "\t\tx = rand(1:6)\n",
    "\t\tprint(x, \" \")\n",
    "\tend\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = ['🍌', '🍎', '🍐', '🍓', '🍒', '🍋'] \n",
    "\t\n",
    "println(\"Pseudorandom numbers generated by rand()\")\n",
    "println(\"----------------------------------------\")\n",
    "println()\n",
    "for i in 1:10\n",
    "    x = rand(1:6)\n",
    "    y = rand('a':'f')\n",
    "    z = rand(fruits)\n",
    "    println(i, \".\\t\", x, \"\\t\", y, \"\\t\", z)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13-5\n",
    "\n",
    "Write a function named `choosefromhist` that takes a histogram as defined in `Histogram - a Collection of Counters` and returns a random value from the histogram, chosen with probability in proportion to frequency.\n",
    "\n",
    "For example, for this histogram:\n",
    "\n",
    "```Julia\n",
    "\tjulia> t = ['a', 'a', 'b'];\n",
    "\n",
    "\tjulia> histogram(t)\n",
    "\t\tDict{Any,Any} with 2 entries:\n",
    "\t\t  'a' => 2\n",
    "\t\t  'b' => 1\n",
    "```\n",
    "\n",
    "Your function should return 'a' with probability 2/3 and 'b' with probability 1/3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Histogram\n",
    "\n",
    "You should attempt the previous exercises before you go on. You will also need https://github.com/BenLauwens/ThinkJulia.jl/blob/master/data/emma.txt. Here is a program that reads a file and builds a histogram of the words in the file:\n",
    "\n",
    "```Julia\n",
    "\tfunction processfile(filename)\n",
    "\t\thist = Dict()\n",
    "\t\tfor line in eachline(filename)\n",
    "\t\t\tprocessline(line, hist)\n",
    "\t\tend\n",
    "\t\thist\n",
    "\tend;\n",
    "\n",
    "\tfunction processline(line, hist)\n",
    "\t\tline = replace(line, '-' => ' ')\n",
    "\t\tfor word in split(line)\n",
    "\t\t\tword = string(filter(isletter, [word...])...)\n",
    "\t\t\tword = lowercase(word)\n",
    "\t\t\thist[word] = get!(hist, word, 0) + 1\n",
    "\t\tend\n",
    "\tend;\n",
    "\n",
    "\thist = processfile(\"emma.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function processfile(filename)\n",
    "\thist = Dict()\n",
    "\n",
    "\tfor line in eachline(filename)\n",
    "\t\tprocessline(line, hist)\n",
    "\tend\n",
    "\treturn hist\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function processline(line, hist)\n",
    "\tline = replace(line, '-' => ' ')\n",
    "\n",
    "\tfor word in Base.split(line)\t\t\n",
    "\t\tword = string(filter(isletter, [word...])...)\n",
    "\t\tword = lowercase(word)\n",
    "\t\thist[word] = get!(hist, word, 0) + 1\n",
    "\tend\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/BenLauwens/ThinkJulia.jl/blob/master/data/emma.txt\"\n",
    "\t\n",
    "filename = open(download(url))\n",
    "\n",
    "em3 = processfile(filename)\n",
    "\n",
    "close(filename)\n",
    "\n",
    "em3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program reads emma.txt, which contains the text of Emma by Jane Austen.\n",
    "\n",
    "`processfile` loops through the lines of the file, passing them one at a time to processline. The histogram hist is being used as an accumulator. `processline` uses the function replace to replace hyphens with spaces before using split to break the line into an array of strings. It traverses the array of words and uses `filter`, `isletter` and `lowercase` to remove punctuation and convert to lower case. (It is a shorthand to say that strings are “converted”; remember that strings are immutable, so a function like lowercase return new strings.) Finally, `processline` updates the histogram by creating a new item or incrementing an existing one. \n",
    "\n",
    "To count the total number of words in the file, we can add up the frequencies in the histogram:\n",
    "\n",
    "```Julia\n",
    "\tfunction totalwords(hist)\n",
    "\t\tsum(values(hist))\n",
    "\tend\n",
    "```\n",
    "\n",
    "The number of different words is just the number of items in the dictionary:\n",
    "\n",
    "```Julia\n",
    "\tfunction differentwords(hist)\n",
    "\t\tlength(hist)\n",
    "\tend\n",
    "```\n",
    "\n",
    "Here is some code to print the results:\n",
    "\n",
    "```Julia\n",
    "\tjulia> println(\"Total number of words: \", totalwords(hist))\n",
    "\t\tTotal number of words: 162742\n",
    "\n",
    "\tjulia> println(\"Number of different words: \", differentwords(hist))\n",
    "\t\tNumber of different words: 7380\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words\n",
    "\n",
    "To find the most common words, we can make an array of tuples, where each tuple contains a word and its frequency, and sort it. \n",
    "\n",
    "The following function takes a histogram and returns an array of word-frequency tuples:\n",
    "\n",
    "```Julia\n",
    "\tfunction mostcommon(hist)\n",
    "\t\tt = []\n",
    "\t\tfor (key, value) in hist\n",
    "\t\t\tpush!(t, (value, key))\n",
    "\t\tend\n",
    "\t\treverse(sort(t))\n",
    "\tend\n",
    "```\n",
    "\n",
    "In each tuple, the frequency appears first, so the resulting array is sorted by frequency. \n",
    "\n",
    "Here is a loop that prints the 10 most common words:\n",
    "\n",
    "```Julia\n",
    "\tt = mostcommon(hist)\n",
    "\tprintln(\"The most common words are:\")\n",
    "\tfor (freq, word) in t[1:10]\n",
    "\t\tprintln(word, \"\\t\", freq)\n",
    "\tend\n",
    "```\n",
    "\n",
    "The tab character ('\\t') is used as a “separator”, rather than a space, so the second column is lined up. Here are the results from Emma; the most common words are:\n",
    "\n",
    "```Julia\n",
    "\tto\t5295\n",
    "\tthe\t5266\n",
    "\tand\t4931\n",
    "\tof\t4339\n",
    "\ti\t3191\n",
    "\ta\t3155\n",
    "\tit\t2546\n",
    "\ther\t2483\n",
    "\twas\t2400\n",
    "\tshe\t2364\n",
    "```\n",
    "Let's check how this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mostcommon(hist)\n",
    "    t = []\n",
    "    for (key, value) in hist\n",
    "        push!(t, (value, key))\n",
    "    end\n",
    "    reverse(sort(t))\n",
    "end\n",
    "    \n",
    "t = mostcommon(em3)\n",
    "\n",
    "#==\n",
    "println(\"The most common words in EM3:\")\n",
    "println(\"----------------------------\")\n",
    "k = 0\n",
    "for (freq, word) in t[1:10]\n",
    "    k += 1\n",
    "    println(\"$k.\\t\", word, \"\\t\\t\\t\", freq)\n",
    "end\n",
    "==#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function clipped_t(hist)\n",
    "\t\n",
    "    k = 0\n",
    "\n",
    "    for j = 1:length(hist)\n",
    "    \n",
    "        if hist[j][1] > 6000\n",
    "            k += 1\n",
    "        end\t\t\t\n",
    "    end\n",
    "return hist[k+1:end]\n",
    "end\n",
    "\n",
    "\n",
    "t = mostcommon(em3)\n",
    "\t\t\n",
    "t = clipped_t(t)\n",
    "\n",
    "println(\"The most common words in clipped EM3:\")\n",
    "println(\"-------------------------------------\")\n",
    "k = 0\n",
    "for (freq, word) in t[1:11]\n",
    "    if word == \"\"\n",
    "        # do nothing\n",
    "    else\n",
    "        k += 1\n",
    "        println(\"$k.\\t\\t\", word, \"    \\t\\t\", freq)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the book:\n",
    "\n",
    "```Julia\n",
    "\t\tto\t5295\n",
    "\t\tthe\t5266\n",
    "\t\tand\t4931\n",
    "\t\tof\t4339\n",
    "\t\ti\t3191\n",
    "\t\ta\t3155\n",
    "\t\tit\t2546\n",
    "\t\ther\t2483\n",
    "\t\twas\t2400\n",
    "\t\tshe\t2364\n",
    "```\n",
    "\n",
    "Why do the results differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\t\t\n",
    "list = sort_on_vs(word_counts(words_em2))[1:N]\n",
    "\n",
    "println(\"List of the $N most common words in \", \"EM2\")\n",
    "println(\"---------------------------------------\")\n",
    "\n",
    "for k = 1:length(list)\n",
    "    println(\"$k.\", \"\\t \", list[k][1], \"   \\t\\t\", list[k][2])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional parameters\n",
    "\n",
    "We have seen built-in functions that take optional arguments. It is possible to write programmer-defined functions with **optional arguments** too. \n",
    "\n",
    "Here is a function that prints the most common words in a histogram:\n",
    "\n",
    "```Julia\n",
    "\tfunction printmostcommon(hist, num=10)\n",
    "\t\tt = mostcommon(hist)\n",
    "\t\tprintln(\"The most common words are: \")\n",
    "\t\tfor (freq, word) in t[1:num]\n",
    "\t\t\tprintln(word, \"\\t\", freq)\n",
    "\t\tend\n",
    "\tend\n",
    "```\n",
    "\n",
    "**The first parameter is required; the second is optional.** The **default value** of `num` is 10. If you only provide one argument:\n",
    "\n",
    "```Julia\n",
    "\tprintmostcommon(hist)\n",
    "```\n",
    "\n",
    "`num` gets the default value. \n",
    "\n",
    "If you provide two arguments:\n",
    "\n",
    "```Julia\n",
    "\tprintmostcommon(hist, 20)\n",
    "```\n",
    "\n",
    "`num` gets the value of the argument instead. \n",
    "\n",
    "The optional argument overrides the default value. \n",
    "\n",
    "If a function has both required and optional parameters, all the required parameters have to come first, followed by the optional ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary subtraction\n",
    "\n",
    "Finding the words from the book that are not in the word list from `words.txt` is a problem you might recognize as **set subtraction**; that is, we want to find all the words from one set (the words in the book) that are not in the other (the words in the list).\n",
    "\n",
    "`subtract` takes dictionaries d1 and d2 and returns a new dictionary that contains all the keys from d1 that are not in d2. Since we do not really care about the values, we set them all to nothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function subtract(d1::Dict, d2::Dict)\n",
    "\tresult = Dict()\n",
    "\t\n",
    "\tfor key in keys(d1)\n",
    "\t\tif key ∉ keys(d2)\n",
    "\t\t\tresult[key] = nothing\n",
    "\t\tend\n",
    "\tend\n",
    "\t\n",
    "\treturn collect(keys(result))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtract(emma_dictionary, wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the words in the book that are not in `words.txt`, we can use processfile to build a histogram for `words.txt`, and then subtract:\n",
    "\n",
    "```Julia\n",
    "\twords = processfile(\"words.txt\")\n",
    "\tdiff = subtract(hist, words)\n",
    "\n",
    "\tprintln(\"Words in the book that are not in the word list:\")\n",
    "\tfor word in keys(diff)\n",
    "\t\tprint(word, \" \")\n",
    "\tend\n",
    "```\n",
    "\n",
    "Here are some of the results from Emma. Words in the book that are not in the word list: outree quicksighted outwardly adelaide rencontre jeffereys unreserved dixons betweens ... Some of these words are names and possessives. Others, like “rencontre”, are no longer in common use. But a few are common words that should really be in the list!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13-6\n",
    "\n",
    "Julia provides a data structure called **Set** that provides many common set operations. You can read about them in `Collections and Data Structures`, or read the documentation at https://docs.julialang.org/en/v1/base/collections/#Set-Like-Collections-1. Write a program that uses set subtraction to find words in the book that are not in the word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function subtract(book::Array{String,1}, list::Array{String,1})\n",
    "\treturn collect(setdiff!(Set(book), list))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtract(emma_words, collect(keys(wordlist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two things to note\n",
    "\n",
    "First, we already used the name **subtract** to define a function given by the signature:\n",
    "\n",
    "```Julia\n",
    "\tfunction subtract(book::Dict, list::Dict)\n",
    "```\n",
    "\n",
    "Here we defined a function also called **subtracte** but with a different signature:\n",
    "\n",
    "```Julia\n",
    "\tfunction subtract(book::Array{String,1}, list::Array{String, 1})\n",
    "```\n",
    "\n",
    "This is out first practical encounter with so-called **multiple dispatch** in Julia. Julia distinguishes between **methods** associated with a given function name and chooses the the appropriate method based on the **input types** in the function call, when appropriately defined in the **function signatures**. Julia announces this by echoing **dictionary_difference (generic function with 2 methods)**.\n",
    "\n",
    "Second, text processing is messy because of a natural lack of standardization and often requires that we operate with Julia's more precise tools, typically all the way down to the character level by using various specifically coded processing tools relevant to the circumstances. Julia's way of dealing with **regular expressions (regex)** is a good place to continue learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random words\n",
    "\n",
    "To choose a random word from the histogram, the simplest algorithm is to build an array with multiple copies of each word, according to the observed frequency, and then choose from the array:\n",
    "\n",
    "```Julia\n",
    "\tfunction randomword(h)\n",
    "\t\tt = []\n",
    "\t\tfor (word, freq) in h\n",
    "\t\t\tfor i in 1:freq\n",
    "\t\t\t\tpush!(t, word)\n",
    "\t\t\tend\n",
    "\t\tend\n",
    "\t\trand(t)\n",
    "\tend\n",
    "```\n",
    "\n",
    "This algorithm works, but it is not very efficient; each time you choose a random word, it rebuilds the array, which is as big as the original book. An obvious improvement is to build the array once and then make multiple selections, but the array is still big. An alternative is:\n",
    "\n",
    "(1) Use keys to get an array of the words in the book.\n",
    "\n",
    "(2) Build an array that contains the cumulative sum of the word frequencies (see Exercise 10-2). The last item in this array should be the total number of words in the book, n.\n",
    "\n",
    "(3) Choose a random number from 1 to n and use a bisection search (see Exercise 10-10) to find the index where the random number would be inserted in the cumulative sum.\n",
    "\n",
    "(4) Use the index to find the corresponding word in the word array.\n",
    "\n",
    "### Exercise 13-7\n",
    "\n",
    "Write a program that uses this algorithm to choose a random word from the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is slightly different approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function clr(wordlist:: Array{String, 1})\n",
    "    cwordlist = Array{String, 1}()\n",
    "\n",
    "    for k = 1:length(wordlist)\n",
    "\n",
    "        if !( wordlist[k] == \"\" || wordlist[k] == \"\\\\\")\n",
    "\n",
    "            push!(cwordlist, wordlist[k])\n",
    "        end\n",
    "    end\n",
    "    return cwordlist\n",
    "end\n",
    "\n",
    "\n",
    "function wordcounts(book::Array{String, 1})\n",
    "    counts = Dict{String,Int}()\n",
    "    for word in book\n",
    "        if word == \"\" || word == \"\\\\\"\n",
    "            # do nothing\n",
    "        else\n",
    "            counts[word] = get!(counts, word, 0) + 1\n",
    "        end\n",
    "    end\n",
    "    return counts\n",
    "end\n",
    "\n",
    "\n",
    "function csum_dict(hist::Dict{String, Int})\n",
    "    cshist = Dict()\n",
    "    cs = 0\n",
    "    for key in keys(hist)        # ks and vs are listed in corresponding order\n",
    "        cs += hist[key]\n",
    "        cshist[cs] = key\n",
    "    end\n",
    "    return cshist\n",
    "end\n",
    "\n",
    "\n",
    "function rnd_wrd(c_hst::Dict)\n",
    "\n",
    "    M = maximum(keys(c_hst))\n",
    "    rnd = rand(1:M)\n",
    "\n",
    "    sc_hst = sort_on_ks(c_hst, false) \t\t# see code just prior to Exercise 13-3\n",
    "\n",
    "    ind = findfirst(k -> r < k[1], sc_hst)\n",
    "\n",
    "    return sc_hst[ind][2]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov analysis\n",
    "\n",
    "If you choose words from the book at random, you can get a sense of the vocabulary, but you probably will not get a sentence:\n",
    "\n",
    "\tthis the small regard harriet which knightley's it most things\n",
    "\n",
    "A series of random words seldom makes sense because there is no relationship between successive words. For example, in a real sentence you would expect an article like “the” to be followed by an adjective or a noun, and probably not a verb or adverb.\n",
    "\n",
    "One way to measure these kinds of relationships is Markov analysis, which characterizes, for a given sequence of words, the probability of the words that might come next. For example, the song Eric, the Half a Bee (by Monty Python) begins:\n",
    "\n",
    "\tHalf a bee, philosophically,\n",
    "\tMust, ipso facto, half not be.\n",
    "\tBut half the bee has got to be\n",
    "\tVis a vis, its entity. D’you see?\n",
    "\n",
    "\tBut can a bee be said to be\n",
    "\tOr not to be an entire bee\n",
    "\tWhen half the bee is not a bee\n",
    "\tDue to some ancient injury?\n",
    "\n",
    "In this text, the phrase “half the” is always followed by the word “bee”, but the phrase “the bee” might be followed by either “has” or “is”.\n",
    "\n",
    "The result of Markov analysis is a mapping from each prefix (like “half the” and “the bee”) to all possible suffixes (like “has” and “is”). Given this mapping, you can generate a random text by starting with any prefix and choosing at random from the possible suffixes. Next, you can combine the end of the prefix and the new suffix to form the next prefix, and repeat.\n",
    "\n",
    "For example, if you start with the prefix “Half a”, then the next word has to be “bee”, because the prefix only appears once in the text. The next prefix is “a bee”, so the next suffix might be “philosophically”, “be” or “due”. In this example the length of the prefix is always two, but you can do Markov analysis with any prefix length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13-8\n",
    "\n",
    "Write a program to read a text from a file and perform Markov analysis. The result should be a dictionary that maps from prefixes to a collection of possible suffixes. The collection might be an array, tuple, or dictionary; it is up to you to make an appropriate choice. You can test your program with prefix length two, but you should write the program in a way that makes it easy to try other lengths.\n",
    "\n",
    "Add a function to the previous program to generate random text based on the Markov analysis. Here is an example from Emma with prefix length 2:\n",
    "\n",
    "“He was very clever, be it sweetness or be angry, ashamed or only amused, at such a stroke. She had never thought of Hannah till you were never meant for me?\" \"I cannot make speeches, Emma:\" he soon cut it all himself.”\n",
    "\n",
    "The result is almost syntactically correct, but not quite. Semantically, it almost makes sense, but not quite. What happens if you increase the prefix length? Does the random text make more sense?\n",
    "\n",
    "Once your program is working, you might want to try a mash-up: if you combine text from two or more books, the random text you generate will blend the vocabulary and phrases from the sources in interesting ways.\n",
    "\n",
    "Credit: This case study is based on an example from Kernighan and Pike, The Practice of Programming, Addison-Wesley, 1999.\n",
    "\n",
    "You should attempt this exercise before you go on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures\n",
    "\n",
    "In your solution to the previous exercises, you had to choose:\n",
    "\n",
    "(1) How to represent the prefixes.\n",
    "\n",
    "(2) How to represent the collection of possible suffixes.\n",
    "\n",
    "(3) How to represent the mapping from each prefix to the collection of possible suffixes.\n",
    "\n",
    "The last one is easy: a dictionary is the obvious choice for a mapping from keys to corresponding values. For the prefixes, the most obvious options are string, array of strings, or tuple of strings. For the suffixes, one option is an array; another is a histogram (dictionary).\n",
    "\n",
    "How should you choose? \n",
    "\n",
    "The first step is to think about the operations you will need to implement for each data structure. For the prefixes, we need to be able to remove words from the beginning and add to the end. For example, if the current prefix is “Half a”, and the next word is “bee”, you need to be able to form the next prefix, “a bee”. Your first choice might be an array, since it is easy to add and remove elements.\n",
    "\n",
    "For the collection of suffixes, the operations we need to perform include adding a new suffix (or increasing the frequency of an existing one), and choosing a random suffix. \n",
    "Adding a new suffix is equally easy for the array implementation or the histogram. Choosing a random element from an array is easy; choosing from a histogram is harder to do efficiently (see Exercise 13-7).\n",
    "\n",
    "So far we have been talking mostly about ease of implementation, but there are other factors to consider in choosing data structures. One is run time. Sometimes there is a theoretical reason to expect one data structure to be faster than other; for example, the in operator is faster for dictionaries than for arrays, at least when the number of elements is large. But often you do not know ahead of time which implementation will be faster. One option is to implement both of them and see which is better. This approach is called **benchmarking**. \n",
    "\n",
    "A practical alternative is to choose the data structure that is easiest to implement, and then see if it is fast enough for the intended application. If so, there is no need to go on. If not, there are tools, like the **Profile module**, that can identify the places in a program that take the most time.\n",
    "\n",
    "The other factor to consider is storage space. For example, using a histogram for the collection of suffixes might take less space because you only have to store each word once, no matter how many times it appears in the text. In some cases, saving space can also make your program run faster, and in the extreme, your program might not run at all if you run out of memory. But for many applications, space is a secondary consideration after run time.\n",
    "\n",
    "The Julia package `DataStructures` (see https://github.com/JuliaCollections/DataStructures.jl) implements a variety of data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging\n",
    "\n",
    "When you are debugging a program, and especially if you are working on a hard bug, there are five things to try:\n",
    "\n",
    "### Reading\n",
    "\n",
    "Examine your code, read it back to yourself, and check that it says what you meant to say.\n",
    "\n",
    "### Running\n",
    "\n",
    "Experiment by making changes and running different versions. Often if you display the right thing at the right place in the program, the problem becomes obvious, but sometimes you have to build scaffolding.\n",
    "\n",
    "### Ruminating\n",
    "\n",
    "Take some time to think! What kind of error is it: syntax, runtime, or semantic? \n",
    "\n",
    "What information can you get from the error messages, or from the output of the program? What kind of error could cause the problem you are seeing? What did you change last, before the problem appeared?\n",
    "\n",
    "### Rubberducking\n",
    "\n",
    "If you explain the problem to someone else, you sometimes find the answer before you finish asking the question. Often you do not need the other person; you could just talk to a rubber duck. And that is the origin of the well-known strategy called rubber duck debugging https://en.wikipedia.org/wiki/Rubber_duck_debugging.\n",
    "\n",
    "### Retreating\n",
    "\n",
    "At some point, the best thing to do is back off, undoing recent changes, until you get back to a program that works and that you understand. Then you can start rebuilding.\n",
    "\n",
    "Beginning programmers sometimes get stuck on one of these activities and forget the others. Each activity comes with its own failure mode. For example, reading your code might help if the problem is a typographical error, but not if the problem is a conceptual misunderstanding. If you do not understand what your program does, you can read it 100 times and never see the error, because the error is in your head.\n",
    "\n",
    "Running experiments can help, especially if you run small, simple tests. But if you run experiments without thinking or reading your code, you might fall into a pattern I call “random walk programming”, which is the process of making random changes until the program does the right thing. Needless to say, random walk programming can take a long time.\n",
    "\n",
    "You have to take time to think. Debugging is like an experimental science. You should have at least one hypothesis about what the problem is. If there are two or more possibilities, try to think of a test that would eliminate one of them.\n",
    "\n",
    "But even the best debugging techniques will fail if there are too many errors, or if the code you are trying to fix is too big and complicated. Sometimes the best option is to retreat, simplifying the program until you get to something that works and that you understand. Beginning programmers are often reluctant to retreat because they cannot stand to delete a line of code. If it makes you feel better, copy your program into another file before you start stripping it down. Then you can copy the pieces back one at a time.\n",
    "\n",
    "Finding a hard bug requires reading, running, ruminating, and sometimes retreating. If you get stuck on one of these activities, try the others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e72a6ea8ae570bc9b562cd7a1d6883761b9cd7ac49ad6b17855b6c636bf0317"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
